{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad10fe87c955413f",
   "metadata": {},
   "source": [
    "# Lab 6: Visualizing the training process of neural networks. Hyperparameter tuning.\n",
    "\n",
    "In this lab, you will learn how to use [wandb](https://wandb.ai/) to visualize the training process of neural networks. We are going to build and train a feed-forward neural network for recognizing handwritten digits of the MNIST dataset. The training process will be visualized in the wandb dashboard, which will allow us to monitor the loss and accuracy of the model in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Feel free to create an account at [wandb.ai](https://wandb.ai/) before starting this lab.\n",
    "\n",
    "### A simple example of how to use wandb in a typical training loop is shown below:\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.login() # Log in to your wandb account\n",
    "\n",
    "# Start a new run\n",
    "\n",
    "some_config = {\n",
    "    'learning_rate': 0.01,\n",
    "    'layer_1_size': 128,\n",
    "    'layer_2_size': 64,\n",
    "    'batch_size': 32\n",
    "} # This is just an example of a configuration dictionary, you can put anything you want here\n",
    "\n",
    "wandb.init(project='mnist-classifier', config=some_config) # start a new run and log parameters\n",
    "\n",
    "# Here you would prepare your data, and initialize the model, optimizer, etc.\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    ...\n",
    "    wandb.log({'loss': loss, 'accuracy': accuracy})\n",
    "    # This will send the loss and accuracy to wandb and you can visualize it in the dashboard\n",
    "\n",
    "# End of the run\n",
    "wandb.finish()\n",
    "```\n",
    "\n",
    "The most important part is the `wandb.log()` function, which sends the data to the wandb dashboard. You can log any metric you want, not just loss and accuracy. The value passed to the function must be a dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c701a2dc778ba9",
   "metadata": {},
   "source": [
    "## Exercise 1: Prepare data for training a mnist classifier (2 points)\n",
    "\n",
    "Before you start training a neural network, you need to prepare the data. In this exercise, you will prepare the MNIST dataset of handwritten digits for training a classifier. You should:\n",
    "\n",
    "1. Load the MNIST dataset using from `data/mnist_train.csv` and `data/mnist_test.csv` files.\n",
    "2. Normalize the data to the range [0, 1].\n",
    "3. Encode the labels using one-hot encoding.\n",
    "4. Create a PyTorch `Dataset` object for the training and test sets.\n",
    "5. Create a PyTorch `DataLoader` object for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73671250bc707ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "import pandas as pd\n",
    "train = pd.read_csv('data/mnist-train.csv')\n",
    "test = pd.read_csv('data/mnist-test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cd2b8d-b86c-4720-9f7e-6c8cf24d41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']\n",
    "X_train = train.drop('label', axis = 1)\n",
    "y_test = test['label']\n",
    "X_test = test.drop('label', axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b077162-9f18-4c96-a0c0-c89d5150ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #could also divide by 255\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#normalizer = MinMaxScaler()\n",
    "#X_train_norm = normalizer.fit_transform(X_train)\n",
    "#X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "#One-hot encoding\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "#Convert into tensors first\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)# convert the features to a tensor\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32) # convert the target to a tensor\n",
    "\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32) # convert the features to a tensor\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32) # convert the target to a tensor\n",
    "\n",
    "train = torch.utils.data.TensorDataset(X_train, y_train)  # create a train dataset object\n",
    "test = torch.utils.data.TensorDataset(X_test, y_test) # create a test dataset object\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198bf53b-bf08-4df9-b302-f6b4489d8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dc7e06a81f23f",
   "metadata": {},
   "source": [
    "## Exercise 2: Prepare the architecture of the neural network (2 points)\n",
    "\n",
    "In this exercise, you will prepare the architecture of the neural network. You should:\n",
    "\n",
    "1. Create a neural network class that inherits from `torch.nn.Module`.\n",
    "2. The neural network should have at least one hidden layer.\n",
    "3. Use ReLU activation functions after each but the output layer.\n",
    "4. Use a softmax activation function in the output layer to get the probabilities of each class.\n",
    "\n",
    "**Feel free to experiment with the architecture of your network** - try adding more hidden layers, changing the number of neurons in each layer, etc. You can also add a dropout layer or some other regularization technique and see if it improves the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05e9629-ec3f-43eb-aee6-91fa00fbb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size_1, hidden_size_2):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.fc3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2efbd440294eb",
   "metadata": {},
   "source": [
    "## *Training PyTorch models on GPU\n",
    "\n",
    "**GPUs are optimized for performing matrix operations in parallel.** Although we call them \"graphics processing units\", they are actually very powerful processors that can be used for any kind of parallel computation, including training deep neural networks. In fact, data science is one of the most common applications of GPUs today, as can be seen by the revenue of companies like NVIDIA over the past few years. NVIDIA is a monopolist in the GPU market - in 2023, the company owned 92% of the data center GPU market share. As for 31 July, the 2024 revenue of NVIDIA was 60.92 billion USD, while the total revenue of 2020 was $10.92 billion. If someone benefits from the current deep learning hype, it is certainly NVIDIA.\n",
    "\n",
    "If you happen to have an NVIDIA GPU in your computer, you can use it to train your deep learning models, as PyTorch has excellent support for CUDA, which is NVIDIA's parallel computing API. To train a model on GPU, you need to explicitly tell PyTorch to move the model and the data to the GPU. \n",
    "\n",
    "Here is an example training loop that uses the GPU:\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # Check if a GPU is available\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "model = SomeNeuralNetwork().to(device)   # Move the model to the GPU\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(device), y.to(device)   # Move the tensors to GPU\n",
    "        \n",
    "        y_pred = model(X)   # Perform a forward pass (on the GPU)\n",
    "        loss = criterion(y_pred, y)   # Compute the loss (still on the GPU)\n",
    "        \n",
    "        ...  # The rest of the training loop\n",
    "        \n",
    "        y_pred = y_pred.detach().cpu()   # Move the predictions back to the CPU to do anything else with them\n",
    "```\n",
    "\n",
    "Note that **the model and all the tensors it uses for computation should be moved to the GPU**. You can do this by calling the `.to(device)` method on the model and the data tensors. If you want to move the data back to the CPU (to process it further, calculate metrics, visualize), you call the `.cpu()` method on the tensor.\n",
    "\n",
    "**Doing calculations on the GPU, you should be wary of few things:**\n",
    "\n",
    "* **The GPU has a limited amount of memory**, so you should be careful not to run out of memory. A typical graphics card has a few gigabytes of memory, so you should be fine with most models and datasets. However, moving very large tensors to the GPU can cause out-of-memory errors. That's one of the reasons why we use a dataloader and process the data in batches.\n",
    "* While the GPU is much faster than the CPU for large matrix operations, **transferring data between the CPU and the GPU is slow**. Therefore, it is best to minimize the number of data transfers between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94574b231d68216",
   "metadata": {},
   "source": [
    "## Exercise 3: Prepare the training loop (2 points)\n",
    "\n",
    "In this exercise, you will prepare the training loop. You should:\n",
    "\n",
    "1. Initialize the neural network.\n",
    "2. Define the loss function (CrossEntropyLoss) and the optimizer (Adam).\n",
    "3. Pass a dictionary with the configuration to wandb. This dictionary should contain all the hyperparameters of our model, including the learning rate, the size of the hidden layers, batch size, etc.\n",
    "4. Train the neural network. Each epoch should consist of a training and validation phase. You should log the loss and accuracy of the training and validation sets using wandb.\n",
    "5. Open you project at [wandb.ai](https://wandb.ai/) and see how cool it is!\n",
    "\n",
    "### Saving and loading the model\n",
    "As training can take some time, it is a good idea to save the model's state dictionary (its learned weights) to a file after training. You can do this with the following code:\n",
    "\n",
    "    torch.save(vae.state_dict(), 'vae.pth')\n",
    "    \n",
    "To load the model from the file, you can use the following code:\n",
    "\n",
    "    vae.load_state_dict(torch.load('vae.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d558333aa5fa0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w12s18pc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-sunset-2</strong> at: <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/w12s18pc' target=\"_blank\">https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/w12s18pc</a><br/> View project at: <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier' target=\"_blank\">https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250127_123311-w12s18pc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w12s18pc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>K:\\Desktop\\PUM\\pum-24\\KSz\\wandb\\run-20250127_123439-5tj4qt3j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/5tj4qt3j' target=\"_blank\">trim-shadow-3</a></strong> to <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier' target=\"_blank\">https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/5tj4qt3j' target=\"_blank\">https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/5tj4qt3j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 1.5895637030601502\n",
      "Validation loss: 1.521342494392395\n",
      "Epoch: 1\n",
      "Train loss: 1.5151518587112427\n",
      "Validation loss: 1.5033317192077638\n",
      "Epoch: 2\n",
      "Train loss: 1.503082183265686\n",
      "Validation loss: 1.4979234630584717\n",
      "Epoch: 3\n",
      "Train loss: 1.4969622621854146\n",
      "Validation loss: 1.4971303491592407\n",
      "Epoch: 4\n",
      "Train loss: 1.4916661415417989\n",
      "Validation loss: 1.4978110719680786\n",
      "Epoch: 5\n",
      "Train loss: 1.4909092624664306\n",
      "Validation loss: 1.4953779731750487\n",
      "Epoch: 6\n",
      "Train loss: 1.4882009740511577\n",
      "Validation loss: 1.4924735137939453\n",
      "Epoch: 7\n",
      "Train loss: 1.486271510219574\n",
      "Validation loss: 1.4971375619888305\n",
      "Epoch: 8\n",
      "Train loss: 1.4855358119010926\n",
      "Validation loss: 1.4906130655288696\n",
      "Epoch: 9\n",
      "Train loss: 1.485067785390218\n",
      "Validation loss: 1.487686209487915\n",
      "Epoch: 10\n",
      "Train loss: 1.4838305391311646\n",
      "Validation loss: 1.4879169128417968\n",
      "Epoch: 11\n",
      "Train loss: 1.4836070056915283\n",
      "Validation loss: 1.4904812147140503\n",
      "Epoch: 12\n",
      "Train loss: 1.4822756112416586\n",
      "Validation loss: 1.4897302452087402\n",
      "Epoch: 13\n",
      "Train loss: 1.4829733205159505\n",
      "Validation loss: 1.4900035884857177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     46\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork(hidden_size_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hidden_size_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss(y_pred, y_batch) \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mbackward()   \u001b[38;5;66;03m# compute the gradients\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# update the weights\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# accumulate training loss\u001b[39;00m\n\u001b[0;32m     27\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;66;03m# compute the average loss\u001b[39;00m\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "import wandb\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs=100):\n",
    "\n",
    "    wandb.login(key = '7fdbfb412f881af911acec90d2685aa03dfc560f') # Log in to your wandb account\n",
    "    wandb.init(project='mnist-classifier') \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #inna funkcja błędu dla klasyfikacji\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "        model.train()   # set the model to training mode (some layers may behave differently in training and evaluation)\n",
    "        train_loss = 0  # this variable will accumulate the training loss\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:   # load data batch-by-batch\n",
    "            \n",
    "            optimizer.zero_grad()   # clear the gradients\n",
    "            y_pred = model(X_batch)  # forward pass\n",
    "            batch_loss = loss(y_pred, y_batch) # compute the loss\n",
    "            batch_loss.backward()   # compute the gradients\n",
    "            optimizer.step()    # update the weights\n",
    "            \n",
    "            train_loss += batch_loss.item() # accumulate training loss\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader) # compute the average loss\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train loss: {train_loss}')\n",
    "         \n",
    "        model.eval()    # set the model to evaluation mode\n",
    "        val_loss = 0    # this variable will accumulate the validation loss\n",
    "        \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            val_loss += loss(y_pred, y_batch).item()    # accumulate validation loss\n",
    "            \n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        \n",
    "        wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = NeuralNetwork(hidden_size_1=128, hidden_size_2=64)\n",
    "train(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e60c6e-7106-4d38-83fa-1832d64c33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: kingasz (kingasz-jagiellonian-university). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kinga.ASUS_KINGA\\_netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8e2772d30f4c8ca5bc8f21d8074072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777932999, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>K:\\Desktop\\PUM\\pum-24\\KSz\\wandb\\run-20250129_115233-ieg2ng91</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/ieg2ng91' target=\"_blank\">swept-valley-6</a></strong> to <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier' target=\"_blank\">https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/ieg2ng91' target=\"_blank\">https://wandb.ai/kingasz-jagiellonian-university/mnist-classifier/runs/ieg2ng91</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork(hidden_size_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, hidden_size_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 51\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# clear the gradients\u001b[39;00m\n\u001b[0;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)  \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[0;32m     25\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()   \u001b[38;5;66;03m# compute the gradients\u001b[39;00m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()    \u001b[38;5;66;03m# update the weights\u001b[39;00m\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mK:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 305, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 235, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 898, in deliver_stop_status\n",
      "    return self._deliver_stop_status(status)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 487, in _deliver_stop_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 452, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 451, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 222, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 154, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"K:\\Gry\\anaconda\\envs\\pum24\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionAbortedError: [WinError 10053] Nawiązane połączenie zostało przerwane przez oprogramowanie zainstalowane w komputerze-hoście\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # Check if a GPU is available\n",
    "\n",
    "# training loop\n",
    "\n",
    "import wandb\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs=100):\n",
    "\n",
    "    wandb.login(key = '7fdbfb412f881af911acec90d2685aa03dfc560f') # Log in to your wandb account\n",
    "    wandb.init(project='mnist-classifier') \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #inna funkcja błędu dla klasyfikacji\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "        model.train()   # set the model to training mode (some layers may behave differently in training and evaluation)\n",
    "        train_loss = 0  # this variable will accumulate the training loss\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:   # load data batch-by-batch\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()   # clear the gradients\n",
    "            y_pred = model(X_batch)  # forward pass\n",
    "            batch_loss = loss(y_pred, y_batch) # compute the loss\n",
    "            batch_loss.backward()   # compute the gradients\n",
    "            optimizer.step()    # update the weights\n",
    "            \n",
    "            train_loss += batch_loss.item() # accumulate training loss\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader) # compute the average loss\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train loss: {train_loss}')\n",
    "         \n",
    "        model.eval()    # set the model to evaluation mode\n",
    "        val_loss = 0    # this variable will accumulate the validation loss\n",
    "        \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            y_pred = y_pred.detach().cpu()\n",
    "            val_loss += loss(y_pred, y_batch).item()    # accumulate validation loss\n",
    "            \n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        \n",
    "        wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = NeuralNetwork(hidden_size_1=64, hidden_size_2=16).to(device)\n",
    "train(model, train_loader, test_loader)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77990efcffab975",
   "metadata": {},
   "source": [
    "## Exercise 4: Easy hyperparameter tuning with wandb (2 points)\n",
    "\n",
    "Wandb allows you to perform hyperparameter tuning by automatically creating multiple runs with different hyperparameters and logging the performance of each run. Below is a brief instruction to `wandb` hyperparameter tuning, but you are more than welcome to find more information in the [official wandb guide](https://docs.wandb.ai/guides/sweeps/).\n",
    "\n",
    "Your task is to use wandb to perform hyperparameter tuning of the neural network, trying different values of the learning rate, batch size, and the size of the hidden layers. You can use the following hyperparameters:\n",
    "\n",
    "First, we need to define a dictionary with the hyperparameters that we want to tune. For example:\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'layer_1_size': {'values': [64, 128, 256]},\n",
    "    'layer_2_size': {'values': [32, 64, 128]}\n",
    "}\n",
    "```\n",
    "\n",
    "Then we need to create a dictionary with the configuration of the run:\n",
    "\n",
    "```python\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'grid',   # grid search, you can also try 'random' or 'bayes'\n",
    "    'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "    'parameters': parameters,   # that's the dictionary with the hyperparameters\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, we can use the `wandb.sweep` function to perform hyperparameter tuning:\n",
    "\n",
    "```python\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')\n",
    "```\n",
    "\n",
    "After that, we can finally run the sweep:\n",
    "\n",
    "```python\n",
    "wandb.agent(sweep_id, function=train)\n",
    "```\n",
    "where `train` is a function that trains the model and logs the metrics to wandb. This function should take a `config` argument, which will contain the hyperparameters of the run. That is how wandb knows which hyperparameters to tune.\n",
    "\n",
    "1. Rewrite the VAE training loop into a function that takes a single dictionary `parameters` as an argument, initializes the model, optimizer, and criterion, and trains the model for a fixed number of epochs. The function should log the loss and accuracy of the training and validation sets to wandb.\n",
    "2. Create a dictionary with the hyperparameters that you want to tune.\n",
    "3. Create a sweep configuration dictionary.\n",
    "4. Run the sweep and monitor the results in the wandb dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f63fb35d55201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train(parameters: dict, epochs = 100):\n",
    "\n",
    "    wandb.login(key = '7fdbfb412f881af911acec90d2685aa03dfc560f') # Log in to your wandb account\n",
    "    wandb.init(project='mnist-classifier') \n",
    "    model = NeuralNetwork.parameters.get(\"hidden_size_1\", \"hidden_size_2\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #inna funkcja błędu dla klasyfikacji\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "        model.train()   # set the model to training mode (some layers may behave differently in training and evaluation)\n",
    "        train_loss = 0  # this variable will accumulate the training loss\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:   # load data batch-by-batch\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()   # clear the gradients\n",
    "            y_pred = model(X_batch)  # forward pass\n",
    "            batch_loss = loss(y_pred, y_batch) # compute the loss\n",
    "            batch_loss.backward()   # compute the gradients\n",
    "            optimizer.step()    # update the weights\n",
    "            \n",
    "            train_loss += batch_loss.item() # accumulate training loss\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader) # compute the average loss\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train loss: {train_loss}')\n",
    "         \n",
    "        model.eval()    # set the model to evaluation mode\n",
    "        val_loss = 0    # this variable will accumulate the validation loss\n",
    "        \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            y_pred = y_pred.detach().cpu()\n",
    "            val_loss += loss(y_pred, y_batch).item()    # accumulate validation loss\n",
    "            \n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        \n",
    "        wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896fac35c516b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'hidden_size_1': {'values': [32, 64, 128]},\n",
    "    'hidden_size_2': {'values': [16, 32, 64]}\n",
    "}\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'accuracy'}, # if we want to maximize the accuracy\n",
    "    # remember to log the metric that you want to maximize or minimize!\n",
    "    'parameters': parameters,\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')    # This will create a new sweep\n",
    "wandb.agent(sweep_id, function=train)   # This will start the hyperparameter tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c411b-0fe6-4a0f-a04d-24be7d989716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
